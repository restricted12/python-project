{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48fe49af-d0b6-4589-92b4-d44969cbe601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page Title: Example Domain\n",
      "Paragraph 1: This domain is for use in illustrative examples in documents. You may use this\n",
      "    domain in literature without prior coordination or asking for permission.\n",
      "Paragraph 2: More information...\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Step 1: Send a GET request to the webpage\n",
    "url = \"https://example.com\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Step 2: Parse the HTML content of the page using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Step 3: Extract the title of the page\n",
    "page_title = soup.title.string\n",
    "print(\"Page Title:\", page_title)\n",
    "\n",
    "# Step 4: Extract all paragraph text\n",
    "paragraphs = soup.find_all('p')\n",
    "for i, p in enumerate(paragraphs, start=1):\n",
    "    print(f\"Paragraph {i}: {p.get_text()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdc6d15a-67d9-4d94-b099-6d2a20ab23d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Link 1: More information... -> https://www.iana.org/domains/example\n"
     ]
    }
   ],
   "source": [
    "# Extract all links from the webpage\n",
    "links = soup.find_all('a')\n",
    "\n",
    "for i, link in enumerate(links, start=1):\n",
    "    href = link.get('href')\n",
    "    link_text = link.get_text()\n",
    "    print(f\"Link {i}: {link_text} -> {href}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d016488-d6b1-410a-8d81-cb9934f4bb09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Headers: ['Year (July 1) ', 'Population', 'Yearly %  Change', 'Yearly Change', 'Median Age', 'Fertility Rate', 'Density (P/KmÂ²)']\n",
      "['2024', '8,161,972,572', '0.87 %', '70,237,642', '30.6', '2.25', '55']\n",
      "['2023', '8,091,734,930', '0.88 %', '70,327,738', '30.4', '2.25', '54']\n",
      "['2022', '8,021,407,192', '0.84 %', '66,958,801', '30.1', '2.27', '54']\n",
      "['2021', '7,954,448,391', '0.86 %', '67,447,099', '29.8', '2.29', '53']\n",
      "['2020', '7,887,001,292', '0.97 %', '75,707,594', '29.6', '2.32', '53']\n",
      "['2015', '7,470,491,872', '1.25 %', '89,751,945', '28', '2.54', '50']\n",
      "['2010', '7,021,732,148', '1.29 %', '86,952,403', '27', '2.60', '47']\n",
      "['2005', '6,586,970,132', '1.31 %', '83,053,428', '26', '2.63', '44']\n",
      "['2000', '6,171,702,993', '1.39 %', '82,564,802', '25', '2.75', '41']\n",
      "['1995', '5,758,878,982', '1.57 %', '86,215,174', '24', '2.89', '39']\n",
      "['1990', '5,327,803,110', '1.82 %', '91,771,929', '23', '3.31', '36']\n",
      "['1985', '4,868,943,465', '1.83 %', '84,267,446', '22', '3.52', '33']\n",
      "['1980', '4,447,606,236', '1.79 %', '75,374,192', '21', '3.74', '30']\n",
      "['1975', '4,070,735,277', '1.96 %', '75,210,297', '21', '4.08', '27']\n",
      "['1970', '3,694,683,794', '2.07 %', '72,030,018', '20', '4.83', '25']\n",
      "['1965', '3,334,533,703', '2.03 %', '63,812,562', '21', '5.07', '22']\n",
      "['1960', '3,015,470,894', '1.93 %', '55,051,420', '21', '4.70', '20']\n",
      "['1955', '2,740,213,792', '1.91 %', '49,424,189', '22', '5.00', '18']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.worldometers.info/world-population/\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find the table by tag <table> or by class if necessary\n",
    "table = soup.find('table')  # You can use soup.find('table', {'class': 'table_class'}) to be more specific\n",
    "\n",
    "if table is None:\n",
    "    print(\"Table not found\")\n",
    "else:\n",
    "    headers = [header.get_text() for header in table.find_all('th')]\n",
    "# Extract rows of the table\n",
    "rows = table.find_all('tr')\n",
    "table_data = []\n",
    "\n",
    "# Loop through rows to extract data from <td> tags\n",
    "for row in rows[1:]:  # Skipping the header row\n",
    "    cols = row.find_all('td')\n",
    "    cols = [col.get_text().strip() for col in cols]  # Clean up data\n",
    "    table_data.append(cols)\n",
    "\n",
    "# Display table data\n",
    "print(\"Headers:\", headers)\n",
    "for row in table_data:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9768ad1-a0a5-44da-a31b-e80c68f89666",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
